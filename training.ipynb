{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LOADING DATA & PRETRAINED MODEL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import pipeline\n",
        "import math\n",
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "device = 'cpu'\n",
        "model_ckpt = 'facebook/bart-large'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_ckpt)\n",
        "\n",
        "dataset = load_dataset('csv', data_files={'train': \"./data/train.csv\", 'test': \"./data/test.csv\", 'validate': \"./data/validation.csv\"})\n",
        "\n",
        "dataset['validate'] = dataset['validate'].select(range(5000))\n",
        "\n",
        "def filter_empty_rows(example):\n",
        "    return all(value for value in example.values())\n",
        "\n",
        "# Filter the dataset using the custom filtering function\n",
        "dataset = dataset.filter(filter_empty_rows)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Data Collator.\n",
        "\n",
        "def get_feature(batch):\n",
        "  \"\"\"\n",
        "  This collarate the content of the inputs to the abstract of the result using the formatted encodings.\n",
        "  \"\"\"\n",
        "  encodings = tokenizer(batch['Content'], text_target=batch['Abstract'],\n",
        "                        max_length=1024, truncation=True)\n",
        "\n",
        "  encodings = {'input_ids': encodings['input_ids'],\n",
        "               'attention_mask': encodings['attention_mask'],\n",
        "               'labels': encodings['labels']}\n",
        "\n",
        "  return encodings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data = dataset.map(get_feature, batched=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [],
      "source": [
        "columns = ['input_ids', 'labels', 'attention_mask']\n",
        "data.set_format(type='torch', columns=columns)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# FINETUNING "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import DataCollatorForSeq2Seq\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir = './model/bart_vietnews_full',\n",
        "    num_train_epochs=1,                         # Number of training epochs. Here, it's set to 1. (>1 leads to longer training time)\n",
        "    warmup_steps = 500,                         # Number of steps for the learning rate warmup.\n",
        "    per_device_train_batch_size=4,              # Batch size per GPU/TPU core/CPU for training.\n",
        "    per_device_eval_batch_size=4,               # Batch size per GPU/TPU core/CPU for evaluation.\n",
        "    weight_decay = 0.01,                        # Weight decay for regularization to prevent overfitting.\n",
        "    logging_steps = 10,                         # Log training information every 10 steps.\n",
        "    evaluation_strategy = 'steps',              # Evaluation strategy to use: 'steps' (evaluation occurs at regular intervals.)\n",
        "    eval_steps=500,                             # Number of update steps between evaluations.\n",
        "    save_steps=1e6,                             # Number of update steps before saving the model. \n",
        "    gradient_accumulation_steps=16              # Number of update steps to accumulate the gradients before performing a backward/update pass.\n",
        ")\n",
        "\n",
        "trainer = Trainer(model=model, \n",
        "                  args=training_args, \n",
        "                  tokenizer=tokenizer, \n",
        "                  data_collator=data_collator,          \n",
        "                  train_dataset = data['train'], \n",
        "                  eval_dataset = data['test'])\n",
        "#"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "trainer.train()\n",
        "trainer.save_model('./model/bart_vietnews_model')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# EXAMPLE USAGE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import pipeline\n",
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "pipe = pipeline('summarization', model='./model/bart_vietnews_model')\n",
        "\n",
        "custom_dialogue=\"\"\"\n",
        "Ngày 7-4, một lãnh đạo Công an TP Hải Phòng cho biết Phòng cảnh sát điều tra tội phạm về ma túy Công an TP vừa triệt phá ổ nhóm \"bay lắc\" tại một khu đô thị cao cấp trên địa bàn phường Thượng Lý, quận Hồng Bàng.\n",
        "Thông tin ban đầu, khoảng 23h30 ngày 4-4, các trinh sát Phòng cảnh sát điều tra tội phạm về ma túy đột kích, phát hiện nhóm \"bay lắc\" gồm 12 dân chơi (7 nam, 5 nữ), thu giữ tại hiện trường 0,13g ketamin cùng một số tang vật khác có liên quan.\n",
        "Kết quả giám định phát hiện 9 trường hợp dương tính với ma túy. Cơ quan điều tra sau đó tạm giữ hình sự ba trường hợp, gồm Nguyễn Thị Thanh Huyền (38 tuổi), Bùi Thị Ngọc Bích (36 tuổi) và Vũ Hoàng Cường (42 tuổi, cùng trú Hải Phòng) để điều tra về hành vi \"tổ chức sử dụng trái phép chất ma túy\".\n",
        "Trong đó Bùi Thị Ngọc Bích là nữ cán bộ đang công tác tại Phòng cảnh sát phòng cháy chữa cháy Công an TP Hải Phòng.\n",
        "Ngoài ra, còn một nữ cán bộ khác là V.A. có mặt tại buổi \"bay lắc\" là cán bộ đang công tác tại Công an quận Dương Kinh, TP Hải Phòng.\n",
        "\n",
        "\"\"\"\n",
        "gen_kwargs = {'length_penalty': 1, 'num_beams': 8,'max_length': 1024}\n",
        "\n",
        "\n",
        "print(pipe(custom_dialogue, **gen_kwargs))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# EVALUATE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ICHzjvFh791I"
      },
      "outputs": [],
      "source": [
        "import transformers\n",
        "import evaluate\n",
        "from transformers import pipeline\n",
        "import pandas as pd\n",
        "rouge = evaluate.load('rouge')\n",
        "gen_sum=[]\n",
        "hum_sum=[]\n",
        "model_name=\"vibart_vietnews\"\n",
        "pipe = pipeline('summarization', model='./model/bart_vietnews_model')\n",
        "gen_kwargs = {'length_penalty': 1, 'num_beams': 8,'max_length': 1024}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import logging\n",
        "\n",
        "logging.set_verbosity_error()\n",
        "gen_sum=[]\n",
        "for sect in dataset['validate']['Abstract']:\n",
        "    gen = pipe(sect, **gen_kwargs)\n",
        "    gen_sum.append(gen[0]['summary_text'])\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for title in dataset['validate']['Title']:\n",
        "    hum_sum.append(title)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "results = rouge.compute(predictions=gen_sum,references=hum_sum)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a new DataFrame from the lists\n",
        "import datetime\n",
        "new_df = pd.DataFrame({\n",
        "    'human': hum_sum,\n",
        "    'generated': gen_sum\n",
        "})\n",
        "x = datetime.datetime.now()\n",
        "time=\"_\".join([model_name,x.strftime(\"%d\"),x.strftime(\"%m\"),x.strftime(\"%Y\"),x.strftime(\"%H\"),x.strftime(\"%M\"),x.strftime(\"%S\")])\n",
        "# Save the new DataFrame to a CSV file\n",
        "new_df.to_csv(\"\".join(['summaries',time,'.csv']), index=False,encoding=\"utf_8_sig\")\n",
        "print(\"Complete: \",results)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "8xPhRF1t7F6t",
        "FEhas0Ui7PP_",
        "Og2k1ZIA7TiF",
        "PwlQnG_c7d_Z"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
